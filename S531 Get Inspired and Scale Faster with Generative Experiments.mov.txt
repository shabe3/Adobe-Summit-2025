00:00:01:31 - 00:00:32:42
不明
Hello, everybody. Welcome to the session. Huge, huge thank you for everybody that, showed up to this session. I know it's the last day of summer, and it's the afternoon and last day summit internally on Adobe. We call this the hangover day. And so we're really, really happy to have you guys and grateful that you aren't. And I think you guys are going to be happy that you came because we have some incredibly cool stuff to show, some stuff that has really drove in a massive amount of impact within our adobe.com infrastructure and new technology that we've been leveraging in a story that has been brewing for the last year or so.

00:00:32:47 - 00:00:51:36
不明
So these are jumping in. I'm not going to go through this and super thoroughly, but I do want to note this presentation is going to be a story that leads to the introduction of a brand new product from the Adobe Journey Optimizer and Target teams. It's going to be really, really cool. And we'll end this with a demo.

00:00:51:41 - 00:01:11:40
不明
But first we're going to do some introductions and I'm going to hand it over to my colleague David. Hi. So I'm David, I'm a research scientist in Adobe Research where I work on experimentation, causal inference in AI, where it suits us. Which you'll hear a lot more about, too. And my name is Paul Elliman. I work for adobe.com, actually.

00:01:11:40 - 00:01:33:50
不明
And, I have quite possibly the coolest job in the world. I, on a team called Personalization Platform. And we spend all day and all year dreaming of massive, huge ideas and leverage the latest and greatest in technology and put them into actual practical use, get them on production and validate them as quickly as humanly possible.

00:01:33:55 - 00:01:56:20
不明
But I didn't do I always do that before I was on the platform team. I was a growth product manager, which, if you're unfamiliar with that term, is somewhere in between a marketer and a product manager. And me and my team. We spent all day and all night and all year running experiments. So we're going to do something fun here today.

00:01:56:20 - 00:02:19:13
不明
We're going to give a really candid look behind the curtain of adobe.com. And, David and I are going to be your guides for this. And as I mentioned before, tell the story here. So the story starts a bit ago when I first joined the growth team that I just mentioned, I was very fortunate to have joined a very high performing team.

00:02:19:13 - 00:02:38:46
不明
And the first year we were operating, we beat our targets. We had we were responsible for driving incremental growth through experimentation. We beat our targets by 70%. We had 100%, 170% of our targets that year, and we were ecstatic. My head was growing big. We were superstars, and we were like the corporate equivalent of being like paraded down the street.

00:02:38:50 - 00:02:58:26
不明
And then of course, whenever that happens, people are like, well, can you do it again? And so the next year we worked really, really hard and we beat our targets again. We had a super high performing team. Not only did we meet that same expectation, we beat it. We doubled our targets that year. So I have a question who in the audience?

00:02:58:26 - 00:03:20:19
不明
Is there anybody in the audience that is really hands on involved in experimentation in the organization? Maybe you're a product manager or marketer or you're, and analytics. Let's see a few people. Yeah. Good amount. So you know what's going to happen next after you beat your targets too many times in a row. And if you have any leaders in the room, they knew that, too.

00:03:21:48 - 00:03:46:43
不明
The next year we got our new targets through experimentation, and they were astronomical. They were more than double what we had ever been able to hit before. Clearly, leaders of our company wanted to challenge us, and at first we were shocked. But then we sat back and we said, well, how would we really do this? And the truth was, under our current tooling, our current process, our current approach to experimentation, it just wasn't possible.

00:03:46:47 - 00:04:06:49
不明
In order to achieve that, we were going to need to do something big. And any time we want to do something big with scale, we think platform. A lot of the times platform is the route to scale. And so we reached out to our, DD partners, our Journey Optimizer partners, our target partners. We reached out to research to learn about the latest and greatest.

00:04:06:53 - 00:04:29:23
不明
And we all started collaborating. And we started with doing a full end to end study on our experimentation process. We wanted to identify the biggest barriers to scale, and we kind of separated up into four big buckets. We have ideation and test design. This is and many of you might be familiar with this, we are identifying opportunities. We are applying that to hypothesis.

00:04:29:34 - 00:04:56:43
不明
And we're organizing it by impact potential to plan our whole year and our whole roadmap and experimentation. The second big bucket is content creation execution, where you take that hypothesis, you take that strategy and you turn it into a web page or an actual experiment. You make the copy, you engineer the experience, etc., etc. then the third step is the actual experimentation runtime or the experimentation lifecycle, where this is the actual duration of the test.

00:04:56:43 - 00:05:21:25
不明
Right? We did a lot of A-B testing on adobe.com and all of our pages, and it would run basically about 30 days on average. We did that to ensure that we were reaching statistically significant result. And then the last thing, of course, is measurement and reporting. We're not only did we do a measurement quantitatively, but we also spent time collaborating and coming up with insights, trying to understand not just what happened, but why it happened.

00:05:21:30 - 00:05:55:08
不明
This funnels right back over to the first step in ideation and test design, where we iterate on that result. Well, we started noticing and defining some really big barriers that prevented us from from achieving that massive amount of skill that we wanted to in ideation and test design. We have a lot of data at Adobe made possible, of course, by our platform so that we have things like app, CGA, Adobe Analytics.

00:05:55:13 - 00:06:25:42
不明
We have so much data, but there was still a problem with scale because even though we had so much data, the best dashboards, the best logic in the world, you still have to take that data and turn it into something central, into a strategy. And that part was very manual and it was very error prone. We had a 30% success rate with our experiments, even though we were such a high performing team, which is kind of low, right from a content creation execution standpoint, you could have the most perfect hypothesis in the world.

00:06:25:53 - 00:06:45:57
不明
You might have nailed the biggest opportunity for your space, but there's still a problem because you have to execute on that. And the content and the, the actual execution of that really, really impacts your test results. So we get to the dev test and would be like, oh, man, this didn't perform very well. Was it because the content wasn't right?

00:06:45:57 - 00:07:14:53
不明
It was a strategy wasn't right. And this continued to affect our ability to get a great ROI from our experiments and a success rate. Then through the experimentation, runtime or life cycle, we did a lot of AB testing in Adobe, and we had many ML tests that helped us scale, but it was very use case specific. And one common misconception that we had was that the problem was that the technology wasn't there and that wasn't the problem.

00:07:14:57 - 00:07:40:40
不明
The problem was more operational. We have 1.5 million pages on adobe.com, of which we only run a very small fraction of tests and experiments on, because it's only practical to do so. AB tests are a lot more simple. They're easier to report on, they're easier to execute. And when you turn that 1.5 million pages into an ML test and it's dynamic and always learning and going nuts, suddenly we have 15,000,000 billion data points that now we need to analyze.

00:07:40:40 - 00:08:10:01
不明
Somehow we don't run enough analysts in the world to help us do that. At the end, we do measurement and reporting. And again, similar to the first step, turning data into actionable insights that we can actually great strategy on that we can execute on that we can learn from is still a manual process that very much relies on the intuition, the skill, the experience of the product managers, the analysts, and everybody who is helping run an experiment.

00:08:10:06 - 00:08:44:24
不明
We started noticing a core problem, a common denominator as a group, we have an abundance of data, no shortage of data, but we are limited in our ability to turn that data into something actionable, both before you set up a test while it's running and after has completed. Because what we really needed to achieve that level of scale at every step of that process was to turn this, this arm at a statistically significant lift, control group, 3% to something more like this, where the audience is probably sensitive and unfamiliar with your brand.

00:08:44:29 - 00:09:12:23
不明
They respond to strategic pricing, creative assets. Here are some creative attributes that correlate across your ecosystem with this audience or this journey. And here are some opportunities suggestions of what to do next. But instead of having that, it was manual and PMS marketers, growth professionals we were all were the ones responsible for connecting these dots between data, insights and opportunities.

00:09:12:32 - 00:09:41:38
不明
How could we truly know what those business opportunities were and how to fully execute to take full advantage of that strategy? It just wasn't practical to do at a global scale, even internally at Adobe, with the best tools in the world. So we started working with our partners, our research partners, our journey optimizer partners, our target partners. In my experience, a lot of really, really big game changing ideas start with somebody to sit around one day and say, man, I want to be great.

00:09:41:38 - 00:09:58:44
不明
If I could do this. And that's exactly what happened here for every step of that funnel, wouldn't it be great if we knew exactly what the biggest insights were in our entire company? Doesn't matter if it happened in my scope, or in another channel or in another country, wouldn't it be great if we just knew what the biggest opportunity was at the time?

00:09:58:49 - 00:10:23:36
不明
Wouldn't it be awesome if, from an execution standpoint, we knew exactly the core visual attributes, the core strategy, the content strategy that would apply to our hypothesis, and what would be awesome if we can test many variants at a time at operational, with operational efficiency, or we can monitor everything, we can see everything very clearly and everything is transparent.

00:10:23:41 - 00:10:50:16
不明
And from an analysis perspective, wouldn't it be awesome if our analysts and our product managers or test leads had the ability to identify these key factors that made up a success of an experiment pragmatically, so they can spend their time being creative and building, more efficiently? The reward of that, if we could do that, would be hearkening back to my earlier story, this exponential, huge scale that we wanted to achieve.

00:10:50:16 - 00:11:15:56
不明
We would have seen increased success rate. We'd win more, we'd get bigger returns on investment, and it would be a lower time investment and less resource intensive. So again, we went to our platform. Teams were all working together and collaborating on this, and we discovered that, hey, the puzzle pieces at Adobe actually did exist already. We just hadn't put them together in the right way yet.

00:11:16:00 - 00:11:38:54
不明
We had the ecosystem. That's one strength of Adobe and our entire digital experience. Platform family. But putting them together in a specific way that we can use in this to solve the scale issue, we had this AI driven content supply chain technology. I'm sure everybody here has heard a massive amount on that. And it was really, kind of revolutionizing the way that we created content at velocity.

00:11:39:06 - 00:12:03:50
不明
We had ML powered experimentation again, target and and now they have features like, target, auto allocate, auto target. They had the capability. And new to the scene was some work that was being done in research and being done at Journey Optimizer around AI, a genetic insight and analysis. For that, I'm going to hand it over to my colleague David Sweet.

00:12:03:55 - 00:12:20:20
不明
Thanks. Yeah. So I guess, that's a lot of buzz word. So it's probably worth like jumping underneath the hood and say, what? What do we actually mean when we say this? And I promise you, there's very little magic. Mostly just math. And on that note, I also want to give a warning to say this is the kind of, presentation that I usually give.

00:12:20:20 - 00:12:36:11
不明
And so this is a little off of my standard tenure. So if it's to high level, track me down after, afterwards, I think Paul can attest to the fact that I love it almost too much to get into the details. And if it's too low level, just, you know, give me some grace. It's the last day for all of us.

00:12:36:35 - 00:13:09:34
不明
Okay. And before I get into the AI piece of this, I do just want to quickly highlight some some previous work, that we've put into production and right now in with research which is around continuous model with confidence sequences. So already if you run an experiment in agile, you can sort of have the advantage of of speeding up the test, your ability to come to a decision from the test due to confidence sequences, which is a way, that replaces standard p values, that lets you continuously peek while maintaining statistical rigor and validity.

00:13:09:39 - 00:13:29:11
不明
So that's great. We can speed up the time to test. But to Paul's point, how do we go past that and how do we scale to so many treatments? You know, we can't tell the number of people we have. So if we tell x number of treatments we have, we need to be a little bit more clever. So to do that, a natural thing to do is to think about using generative AI.

00:13:29:16 - 00:13:47:24
不明
Right. And so, we can sort of think about experimentation and AI in terms of two paradigms. So on the left we have, we have the two sort of like titans of these two things. On the left we have Jeff Hinton, on the right, we have someone named Jerzy Neyman. And we can think about the pros and cons and why we, why we find each appealing.

00:13:47:29 - 00:14:12:19
不明
So first we can think about why we like AI so much. It's great. Our bosses love that we use the buzzword, but in practice, the reason it's so nice is we get really good empirical behavior on a wide range of settings, with little to no examples. This is almost in direct contrast to statistics and causal inference, where if I want to make a decision or if I want to make a conclusion, I require myself to have a lot of data and that can be very burdensome.

00:14:12:24 - 00:14:34:11
不明
But I think those of you who are maybe a little more cynical or clear eyed in the audience may know that this comes with no free lunch. Because on the other side, the thing that you don't get from generative AI typically is rigorous guarantees, reliable uncertainty estimates and decision criteria. And when we do experiments and things like causal inference, this is essentially exactly why we're showing up to the table.

00:14:34:13 - 00:15:02:15
不明
We need to have the kind of guarantees that we can make decisions on top of, such that in a month, if something happens, we need to say these were the criteria we had is based on really rock solid math. So essentially the work that we've been doing is asking how we can marry these two paradigms. So what we've done is sort of take statistics and machine learning that's explicit, designed to leverage AI based inference while providing guarantees that are no worse than their classical counterparts.

00:15:02:19 - 00:15:18:30
不明
So that's a lot of sort of word salad. Another way of saying that is, you make some assumptions when you use generative AI. And if those assumptions are wrong, your worst case behaviors are running the AB test you thought you were running when you showed up. So if you would basically what you would have been doing before.

00:15:18:35 - 00:15:37:35
不明
But if it goes right, then you get to have sort of an increase in precision and increase in sort of like the ability to do these things. Okay. So put this more into context and maybe harken back to, to Paul's notion of what that cycle means. We can think about the experimentation process in sort of three distinct phases.

00:15:37:40 - 00:15:55:53
不明
The first one is, is insights, right. This this is essentially, hey, I should run an experiment, right? We come up with it with a set of of things. We think we can measure. Right. This is the basically what what marketers get paid for, right? You all have great ideas about what's going to change. They can think about opportunities and opportunities is exactly what you're going to change and how you're going to change it.

00:15:55:58 - 00:16:12:57
不明
Another way of saying is these are the variants that you run in your AB test, your bandit, whatever. And then finally we have performance and performance performances. I know what I want to run, give me a really precise estimate and give it to me as fast as possible so I can run my next experiment quickly so I can make this decision and and sort of get on with life.

00:16:13:02 - 00:16:33:22
不明
Okay. So with with these three things, you may be asking, okay, how are we actually changing how we run these things, like what's what's fundamentally different here? Well, I think it's really useful to anchor on what we typically do in a B test. Right. So when you typically run an AB test. So here and throughout the rest of my, my little section here we use images because they look nice on slides.

00:16:33:22 - 00:16:54:10
不明
But just know this also applies to things like text and layouts. It's just I don't want to bore you guys in the last session of the day. But let's say you're running your your AB test. You have these sort of three here images. You treat these things as independent. Another way of saying that is you're assuming you have no information about these things other than the fact that I have treatment A, B, and C, okay.

00:16:54:22 - 00:17:23:43
不明
But if I asked you which which are two of these are going to perform similarly, right. You're probably going to do something fundamentally different, which is you're going to rack your brain. Maybe you're going to hearken back to, you know, your early days of television. You say some of these things look more similar than the others, right. And so what we can sort of take that same intuition and operationalize it with generative AI and embeddings in order to start thinking about how we can relate similarity between treatments.

00:17:23:43 - 00:17:45:16
不明
So we can explicitly leverage those similarities and make our treatments more efficient when we go to run the experiment. But more importantly, use that sort of similarity and information about our treatments to draw insights and help us make better hypotheses. Okay, so let's do that a little bit more concretely. So here let's imagine that I have sort of two embeddings.

00:17:46:24 - 00:17:59:27
不明
Dimensions. One is how stretchy things are. One is how bright things are. And what I'm going to do is I'm going to take I'm going to take all of my, my variants. I'm going to place them in the space. Okay. And I can do that for a bunch of stuff. Right. And we know we do this all the time.

00:17:59:27 - 00:18:20:14
不明
Right. And one of the key things here is that we can generate a ton of these sort of semantic embeddings or sort of meaningful embeddings. And really what we're doing is we're just taking that same insight that you bring to the table when you think about, hey, what's going to work right? This exact same logic that you're doing, the only thing we're doing is sort of making it mechanical, right?

00:18:20:14 - 00:18:39:13
不明
And pushing it through the machinery of, of statistics. And then once we have these things, something very subtle changes. It's really important. So instead of saying how are these variants doing? Instead we're essentially looking at the space and saying, how are stretch your things doing? How are brighter things doing? So we're not thinking about like discrete treatments anymore.

00:18:39:13 - 00:18:56:47
不明
We're thinking about the attributes of their treatments and how they're affecting the things we care about. Which means that when we want to add a new treatment with this dapper young man here, we can go back to our our embeddings and we can say, well, I think it's actually going to perform very similar to other stretchy, great things.

00:18:56:52 - 00:19:17:52
不明
Let's us do a couple of things. If we go to run the experiment, we can sort of partially pull information from other similar treatments in a way that increases the power. So to improve the precision of that let you finish quicker, but also gives you a sense before you run the experiment. Hey, this is maybe more likely to do well because we know that other stretchy and bright things tend to have done well.

00:19:18:00 - 00:19:38:56
不明
We can also flag things like, you know, this is like the 15th time you've run this same yellow zip, right? Like which which sounds kind of silly, but I think across organizations, you know, a lot of times you'll sort of look at historical data sets and you'll see maybe one word changes, you know, commas different. And so it kind of helps to, to make that a little bit more explicit here.

00:19:39:01 - 00:19:59:16
不明
Okay. So now now that we have that we can think about other ways we might think about what a good treatment is. So I could ask you just quickly rack your brain like what's a good treatment. I'm going to give you my, my our thoughts on what that is. So the first one, like I mentioned is now if we run something similar to this in the past, okay, we can also think, are there similar things that have or haven't worked well?

00:19:59:16 - 00:20:26:46
不明
Again, thinking about those, those attributes, just the sort of categorizations. But we can add additional information here. Like does this fit our brain guidelines? Seems kind of fundamental. Something probably want to think about for a run. It does this fit our company goals? Right. As a as another brand run a similar experiment. Right. And maybe the most important question that you could ask before running any sort of experiment is, is my boss going to like it so we can we can operationalize a lot of these things.

00:20:26:46 - 00:20:51:59
不明
Unfortunately, we can only implicitly model the last one. But I think you all are smart and can come up with some surrogate metrics. And what we do is we, we take sort of those surrogates for each one of those things. So you see on the left here, you have things like converting attributes, the previous learning. So that's explicitly taking your previous experiments and and leveraging them in data set to sort of use it as opposed to just sort of keeping that locked inside of our brain.

00:20:52:04 - 00:21:09:49
不明
What metrics do we have available? What have our company goals? We can sort of like, you know, write that down and use it inside of our model. And also our, our best practices, both industry best practices and also internal best practices. You put this inside our, AI experimentation opportunities agent, which is a bit of a mouthful, but bear with me.

00:21:09:55 - 00:21:28:34
不明
So I stats, model. And then what we get is a couple things. One, we see a hypothesis here. Right. So so this is the treatment you're going to try. I've switched over to text because I like sleight of hand. So say this is, this is a headline here that maybe Paul might be running. This says, you know, upload creativity in your apps today.

00:21:28:34 - 00:21:53:02
不明
Right. But you'll notice something else, which is why this might work. And this is explicitly using those attributes that I just talked about and your prior historical data, your previous, experimental results in a way that sort of makes explicit, gives you a natural, natural language explanation of, the hypothesis that you're running, you know, the headline with a neutral tone that's short and direct would increase sales.

00:21:53:07 - 00:22:12:13
不明
I think a lot of times we do this, but it's useful to write it down explicitly for two reasons. One, when you're generating things with a model, it's nice to know why you're seeing what you're seeing and you're not just seeing 100 variants at once, because that's both overwhelming and sometimes just feels like white noise. And the other version is you get to keep this.

00:22:12:28 - 00:22:29:00
不明
So later when you go to look at your result, you can know why you started it. Right? A lot of times we forget why we thought good ideas were good ideas. Okay, so we have those things. Now we're going to run it. Say we give you some examples. You're super excited. Now what do I do? All right.

00:22:29:05 - 00:22:51:57
不明
I'm not sure how sort of streamlined and, well-documented all your systems are, but most places that I've sort of seen and then have the following process. You take your result, you hand it off to an analyst, they disappear to a dark room for weeks, months, maybe the quarter. You send them some really panicked email saying like, we got to tell people what happened soon.

00:22:52:02 - 00:23:08:25
不明
They come back with a presentation, usually very high quality because you like them. And they say, hey, this is what happened. This is why it happened, right? You guys sort of like brainstorm and this is what we should do next. Awesome. You're super psyched. And then what do you do? You you put it into cloud storage and then it dies.

00:23:08:25 - 00:23:24:29
不明
And then maybe four months later, someone says, hey, we think it's the experiment. You're like, we actually run that exact same experiment like four months ago, or maybe not, right? Like maybe we just forget and you just run the same thing again. So one of the things we'd like to avoid is this. Because what you're really doing is you're losing the basic value of experimentation here.

00:23:24:35 - 00:23:47:09
不明
You don't really run an experiment to find out what's going to happen in one iteration, which you're really trying to do is build sort of a base of knowledge and understanding based on experiments that let you sort of inform your strategy and your intuition, not just for you, but organizationally. So you can send it to your director, whoever, and you all can learn together in a way that that feels based in science.

00:23:47:14 - 00:24:07:40
不明
Okay. So to do that, we use insights, for for our purposes, we're going to define an insight as a human readable statement of evidence that helps to update and refine hypotheses. Again, I really like long statements, but I can say this in the short away, this should either help tell you what to do next, or it should help you explain what you just did.

00:24:07:45 - 00:24:21:49
不明
If it's not doing either one of those things, it's not an insight, it's just words. So this this is what we're trying to try to this is what we do in our product. How do you find these insights? Well, we can go back to what is quickly becoming my favorite graphic. And we can think about this space that I just showed you.

00:24:21:49 - 00:24:44:23
不明
Right. And we can find places where things did very well. I have circled in green. We can find things where things did poorly. We looks there's light, there's light in bright spots. And we can look at that across all of those categories. Now we have sort of attributes that are associated with good and bad performance. And then we can take that and sort of surface it back to you and say, hey, these are the kind of things that are working well, okay.

00:24:44:34 - 00:25:06:57
不明
So what does this look like? Let's think about, three different sort of levels of complexity here. So the first one is probably what we all do hopefully when we run an experiment, which is that, you know, this, there's strong evidence that exactly the experiment I ran worked great. So this this correlates to hypothesis that we started with, which is that we think that bright colored products converged better on the homepage.

00:25:07:02 - 00:25:30:51
不明
Wonderful. We get a little more nuanced, though, and we can think about an insight that narrows eyes on specific attributes that they talk like they talked about. And now we're we're thinking not just about that specific experiment, but also what it is about the content that you're actually experimenting on that worked or didn't work. And then finally, we can think about going down another level and not just talking about the attributes, but also your audience.

00:25:31:00 - 00:25:52:04
不明
Who is this working for? Right. And we can think of the intersection of these things. Right. These are the specific strategies or attributes that work for this, these segments of my my customer base. And then over time, what you do is instead of just having a bunch of experiments that you ran and threw away, you're building up an insight base so you can sort of learn from these organizationally, you can surface this other people.

00:25:52:14 - 00:26:09:08
不明
And also when you go to run your next experiment, like I talked about before, we had that sort of ranking mechanism. And that's based on exactly that, that sort of safe knowledge that we talked about. So every time you run an experiment, you're not just running an experiment, you're making the next experiment better and sort of improving your decision making process.

00:26:09:13 - 00:26:25:09
不明
So now that I've rambled for a solid ten minutes, I'm going to hand it back over to Paul where you can. Oh, before I do that, I can't help myself. One second. I want to give you a quick, quick moment under the hood, which is just to tell you that this really isn't smoke and mirrors. Here are the two.

00:26:25:09 - 00:26:42:55
不明
The broad technical ways they do it. So I really can't help myself. It, the first is, we use embedding generations that are sort of, two levels. So we use embeddings and we also use something called Now let's go semantic embeddings. So we have embeddings that like I was talking about before that relate to specific attributes that we care about.

00:26:43:13 - 00:27:07:05
不明
Then we have a hypothesis ranking selection model, that, that leverage embeddings and prior experiments. We do a little bit of work there under the hood to account for things that might bias specific experimental results, like, hey, I ran this in Christmas, like kind of stuff. So a little bit extra statistics. And then finally we have hypothesis description characterization, which is a process where we come up with attributes that we define.

00:27:07:05 - 00:27:25:31
不明
And also, that customers can define as well, that allow us to sort of have have a bank of attributes that we're really trying to sort of recent over. So with that, I'll finally, hand it back over to you, Stephen. Okay. So I'm sure everybody here thinking, wow, that makes a lot of sense because it really does.

00:27:25:31 - 00:27:45:05
不明
It's actually not that complicated. Meaning that is very complicated. But but at a high level it's not very complicated. Right. But there's always this disconnect that I noticed which is like, okay, we have a really cool thing, but how do we take that into actual practical use? Right. How do we work that into our day to day, our existing experimentation process?

00:27:45:10 - 00:28:14:39
不明
And again, remember, we're trying to achieve in this year in my story, an immense amount of scale quickly. And my team, the personalization platform team is extremely, passionate about putting them into practical use, validating, incubating that quickly. So this brings us back to about a year ago, actually. We actually started building an internal tool@adobe.com with the Journey Optimizer team that, would resolve some of these big user problems.

00:28:14:39 - 00:28:45:36
不明
And we started kind of sneakily using it. Right. And in all my time of being@adobe.com, you know, as a product manager in the growth profession, I have never seen anything as enthusiastically and overwhelmingly adopted as quickly as the work that we did in this internal tool. And so to give you a better idea of how exactly we're doing this and spoiler alert, this ends in a new product for Adobe, I'm going to show you a case study of an actual experiment that we run or we ran within Adobe.

00:28:46:00 - 00:29:03:06
不明
Dot com. This is the Creative Cloud overview page is what we call it internally. And we would typically do this a very high performing page. It a lot of people come through this and it's a big marketing page and generates a lot of revenue for the company. And typically we do AB tests on this page.

00:29:03:06 - 00:29:23:30
不明
Right? We might say the marquee. The marquee is a really high visibility page. And we really want we think that's a massive opportunity. So we do an AP test. But as an AB test, really what we want to do. So not really. That's the method that we're trying to do what we want to do. What we really want to do is we want to find the most performant header.

00:29:23:39 - 00:29:51:26
不明
We want to find the most optimal subhead, we want to find the most engaging CTAs. And we also want to understand does that differ by audience? Does a user behavior between audiences differ what they respond to? They respond to? The answer is probably yes. So remember back when I was talking through the entire, timeline here, this at minimum 30 days to get a clean read, 30 days for the header, 30 days for the subhead, 30 days for the CTA.

00:29:51:26 - 00:30:23:34
不明
Now you're at a full quarter and you've used up a quarter of your roadmap. And on top of it, if you layer on audiences now, you times that by for now, you're at 360 days of not only run time, but 360 days of collaboration across multiple multiple teams. Extremely, extremely expensive and not particularly scalable. But now using that internal tool that we had developed, we had tools to do this, this Creative Cloud overview page tests.

00:30:23:39 - 00:30:49:25
不明
Instead of defining an A versus B, we utilize technology from targets, auto allocate to define a pool of content. We're able to say a wide breadth of strategies. Machine tell us which one of these works organically and report back to us for each one of our audiences. And this is unfortunately, this is, not actual data, but, just from confidentiality perspective.

00:30:49:25 - 00:31:17:48
不明
However, I can tell you that this was a massive win for the company. This was already a pretty mature surface for the company, and we had double digit revenue growth from this page. We vastly increased the business impact, but more importantly, we vastly improved the user experience. It was more personalized and more relevant than ever. Now here's the really special part that relates it back over to what David was talking about.

00:31:17:53 - 00:31:50:26
不明
In addition to that, we were building an agent with Adobe Journey Optimizer that reports back to us in a UI. So not only are we able to use ML, but we're able to operationalize it. It continuously listens. It suggests what to do next and why an experiment, or why an experience is performing the way it is by connecting those points between performance data, quantitative data and, the, the AI work that that David did.

00:31:50:26 - 00:32:18:15
不明
So, combining strategy with data, something that was typically done very manually. And we did it on a grander scale. This is the real magic here, something that was literally impossible before. Not only does the experiment agent continuously listen to your experiment in your space, in your context, it listens to the entire ecosystem of Adobe Journey optimizer of app of Adobe Analytics that Adobe is on.

00:32:18:15 - 00:32:43:38
不明
And it's able to understand, oh, what happened in what worked in email, what happened in our in-app surfaces on Photoshop. You know, because the next big thing for you and your focus, your line of business, it might be from the page in Japanese on the photography play, page from Japan. And that insight gets now organically worked into the experimentation process.

00:32:43:43 - 00:33:09:15
不明
Where in our UI, we get it surfaced seamlessly in the experience, something that we can explore, we can judge as humans who are experts in the space and execute on very, very quickly. And like I said, this was about a year ago and it had an incredible amount of adoption@adobe.com. Throughout the remainder of the year, we ran many, many tests.

00:33:09:15 - 00:33:32:42
不明
We basically like ripped out every possible personalization and content test that we could to transition to this because it was so much more effective. We saw through our AI driven content supply chain, over 200% average increase to the amount of content we could create. We saw a 24% relative increase to our win rate or success rate, and a 212% average ROI protests.

00:33:32:42 - 00:33:54:20
不明
In our case, we were focused on revenue growth. So put in plain English, we were able to test far more with the same amount of resources, and we were able to not only test far more, but when we did get a winner, it was a larger winner and created more business impact. So we're all clinking our champagne. And by the way, we hit earlier that year.

00:33:54:26 - 00:34:28:06
不明
So that was nice. And, we're all sitting around patting ourselves on the back and we thought about something like, what if like, other people have the same problem that we resolved internally at Adobe? And so we with Journey Optimizer, we got together and started talking to some people, and I think there's a couple people in this room that we talked to and decided we were going to collaborate on an new Adobe product, and I've been looking forward to the last year to stand up on the stage and could tell you guys this.

00:34:28:11 - 00:35:04:34
不明
So introducing the Adobe Journey Optimizer Experimentation Accelerator. This is a new gen. I have first product that, that is under the Adobe Journey Optimizer banner. And using this application, you can do the exact same work that we just described, taking advantage of the experiment agent and all of the, workflows that we've been using adobe.com, it helps you design the best possible tests, that helps us design the best possible test by automatically surfacing and correlating those attributes that correlate with performance in your context.

00:35:04:39 - 00:35:26:17
不明
We were able to test more variations of time, and you can do that now here and again we have that technically, but I will soon show you that it's more up analyzable than ever. In addition, the experiment agent is plug and play and on continuously analyzes connects the dots between data and strategic insights in plain English so you can execute on them.

00:35:26:30 - 00:35:56:44
不明
And it identifies the biggest opportunities dynamically learning from your entire ecosystem and applying them strategically to what you're working on. And one of the coolest parts, in my opinion, is that it seamlessly integrates with both Journey Optimizer and Target. It doesn't take any additional integration, it works out of the box and communicates with those. You're basically just plugging and playing the, the experiment agent and the UI into.

00:35:56:58 - 00:36:34:02
不明
Whether you're a journey optimizer customer or a target customer, you'll have access or you'll be able to get access to this functionality. Okay, that was a lot of preposition, but we're going to do a demo now. So@adobe.com, this is the Experimentation Accelerator. This is our home page for product managers, for marketers, for adults. Anybody that touches experimentation and wants to take advantage of these features up top, we have a very high level view of the metrics that are driving our business that we really, really care about.

00:36:34:07 - 00:37:05:28
不明
Down below here we have additional metrics L1, L2, KPIs that we track. Again, measuring business. Down here you can see that there are some new opportunities being surfaced organically in this process. I'm going to hit that in a moment. We also the experiment agent is recognizing big milestones, important things in this section down here that might be relevant to you, that you should probably check out because it's driving a lot of business, or it's a major milestone in the organization.

00:37:05:28 - 00:37:29:45
不明
And down here again, another browsable, another discovery, section, you're able to see all the top performing experiments, and we could see all the experiments running on adobe.com now. But we're not just any adobe.com. Product manager or test lead. We were actually at Adobe. We're gonna pretend like we're a Photoshop product manager, and so we can switch to what we're calling, we're kind of categorizing as a team space.

00:37:29:49 - 00:37:55:52
不明
And so this is customized just for me being a Photoshop practitioner. And the experiment agent is doing separate things here. It is analyzing everything the same as at all, but it's really applying it to your context. So this can be your home page for for the things that you really are focused on. Let's actually click into one of these core metrics here.

00:37:55:57 - 00:38:18:52
不明
On the core metrics page, we see a high level view of kind of like how the last year is going. Again, the experiment, the experiment agent is, recognizing big milestones and strategy and some things, not just strategy, just some regular utilitarian things, like a test was launched or a test one, we see a list of experiments, year to date that are impacting this metric that you can click in and browse.

00:38:18:52 - 00:38:45:19
不明
We're going to look at that in a moment and over to the right in the opportunity section is where a lot of the magic is happening. Because organically in your process, the experiment agent is recognizing when there is an insight or when there's something critical that could be your next big test. If we click into that, we can see details about this insight, and we try to be as transparent as possible, which is absolutely critical.

00:38:45:24 - 00:39:20:10
不明
We call out where this insight came from. We call out not just what the insight is, highlighting Photoshop's distraction removal feature, but we also the experiment agent also explains why it thinks that this is working, so you can make the judgment call as a practitioner yourself down here in the, improvement, the, improvement this quarter section, this is basically an aggregate, an average of what, this, new treatment, what this messaging strategy, how it can, how it on average, impacts the business, the business metrics.

00:39:20:10 - 00:39:38:01
不明
And in this case, it's 3%. And I might have just logged in here. I saw this opportunity is like, oh man, that's a really great idea. I want to add this to my experiment. And you can click it here. So this might look different for every organization. But some of some people have like separate teams. They have martech teams.

00:39:38:01 - 00:40:01:23
不明
They have an engineering team that, you know, you might want to hand it over at this point and add different pieces of content to take advantage of the opportunity. Maybe our name sites, customer, but we're also building the capability and we're using we're using a, a kind of prototype version of this internally at Adobe that allows you to generate and brainstorm an idea, eight different potential executions.

00:40:01:23 - 00:40:16:54
不明
And this is a copy test, but it can do imagery and layout and everything. And, you know, we find ourselves using this as a really critical brainstorming tool. It gets us over that cold start creative process, but you don't have to use it. In this case, we're going to use it and we're going to use this content regardless.

00:40:16:59 - 00:40:43:48
不明
It shows up organically inside of how that handoff is seamless. And now you've just added something to your new pool of content. So let's go back and imagine that sometime this past maybe two weeks or so, down here in the A notification section, we see that our test has a new treatment that is winning. This is what we call the experimentation or the experiment detail page.

00:40:43:48 - 00:41:01:10
不明
This is our experiment. This is a point of a journey, perhaps for a specific audience. And that was relates to the content that we just added down here. When we scroll down, we can see kind of like a leaderboard and a huge motorsports fan. And, I have this open every single day on my desktop for the things that I'm running.

00:41:01:10 - 00:41:21:33
不明
It's actually fun tracking and seeing what the AI is doing and why it's doing it. And that sounds like magic, but it's really not that complicated. The AI is slowly funneling more users into higher performers and slowly funneling and gradually, eliminating content or eliminating variations of any experiment that are not performing. Well. It's as simple as that.

00:41:21:42 - 00:41:47:40
不明
Then it does it with statistical significance, of course. Again, if I wanted to say add another thing to continue this, this, this process, I can go into the opportunities and add more. This creates that really tight scale that we struggled with so hard and also very seamlessly integrates AI, a genetic work, into our experimentation workflow in a way that doesn't feel separate.

00:41:47:40 - 00:42:05:28
不明
It doesn't you don't have to take us, a specific action for it. It's organically understanding your context and suggesting things for you. This creates a loop that we're taking advantage of. And by the way, I showed an example of one of those ML tests that we're working on. But it also supports AB testing in the same way.

00:42:05:33 - 00:42:32:39
不明
So yeah. Going back to the presentation, some final thoughts. And and so final thoughts. One thing that I love about this concept, and the one of the reasons why my colleagues are so excited about it as well, is this was born and bred, has initially some really small internal tool. It was built to solve a user problem, a real one that we're going and we didn't start like, oh, let's build something with AI.

00:42:32:44 - 00:42:53:58
不明
We didn't like that at all. We were we had a way less inspirational goal. We were like, oh man, we need to hit our targets. How are we going to do that? We have we need another tool. We need something to help us do that. And we just so happened to have stumbled up on the agenda AI stuff that we that we organically worked into, a process that we knew that we needed, and that collaboration was only made possible by the Journey Optimizer team.

00:42:53:58 - 00:43:00:35
不明
By the way, which we have worked very closely with them on all of this.

00:43:00:40 - 00:43:30:06
不明
The new, the new loop is, is very strong within adobe.com. We are able to design the best possible tests. Now we can quickly take action on those big opportunities, and they're automatically identified and surfaced for us at scale. We can execute with a lot higher confidence than before with the experiment agent calling out strategic and attributes that correlate with performance that can help you and your strategy specifically succeed in your environment.

00:43:30:32 - 00:43:48:13
不明
We're able to execute much more effectively and collaborate with our design teams or copywriter teams. We can test more in less time. We have this beautiful UI by the Journey Optimizer team that helps us monitor these tasks, helps us keep track of them. It's actually kind of a delight to go in and look at how a test is doing.

00:43:48:20 - 00:44:10:00
不明
We can also analyze on this global scale, a lot of the heavy weight is taken off of the manual. Let's, you know, attend every single meeting we can and write things down. Hopefully don't. Hopefully we don't forget them. And that is creating this next generation loop within Adobe Ecom where we have this radically scalable made possible by that gen AI enhanced experimentation.

00:44:10:05 - 00:44:31:30
不明
Okay, so I think we did pretty well on time today. And one thing that I do want to note is that we are soon going into beta here. And if anybody is interested in taking part, there is a QR code here where you can sign up for, for the beta. There is there are two requirements. The first requirement is you must have target or journey optimizer.

00:44:31:30 - 00:44:48:50
不明
You must be customer already, and you must be open to giving feedback as well. But you know, even if you are not ready to sign up for the beta, we're always open to talk and chat. David and I are massive nerds, clearly. And I just we all love talking with. And I'm judging optimizer team looking at them too.

00:44:48:52 - 00:45:03:26
不明
And they're also insane nerds. So if you want to talk to me, just nerd out for a little bit. Open to doing that too. And taking your feedback is always appreciated as well. Feel free to reach out, sign up for the beta. Even if you just want to chat and brainstorm and nerd out with us. Open for that too.

00:45:03:31 - 00:45:12:45
不明
And, we'll be here for a little bit longer. And in this phase and in this room. And then we'll go downstairs to the booth that's that's there, and feel free to visit and let's, let's talk.

